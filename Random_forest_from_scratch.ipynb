{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Algorithm Implementation\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate accuracy of the model\n",
    "def accuracy_score(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# split a dataframe based on feature and feature value\n",
    "def split_node(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor col in dataset:\n",
    "\t\tif col[index] < value:\n",
    "\t\t\tleft.append(col)\n",
    "\t\telse:\n",
    "\t\t\tright.append(col)\n",
    "\treturn left, right\n",
    "\n",
    "# Calculate the gini index for a best split dataframe\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at get_split point\n",
    "\ttotal_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [col[-1] for col in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / total_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Select the best split for a dataframe\n",
    "def best_get_split(dataset, k_features):\n",
    "\tclass_values = list(set(col[-1] for col in dataset))\n",
    "\ttree_index, tree_value, tree_score, tree_groups = 9999, 9999, 9999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < k_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor col in dataset:\n",
    "\t\t\tgroups = split_node(index, col[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < tree_score:\n",
    "\t\t\t\ttree_index, tree_value, tree_score, tree_groups = index, col[index], gini, groups\n",
    "\treturn {'index':tree_index, 'value':tree_value, 'groups':tree_groups}\n",
    "\n",
    "# Create a terminal node \n",
    "def terminal_node(group):\n",
    "\toutcomes = [col[-1] for col in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def get_split(node, max_depth, min_size, k_features, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no get_split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = terminal_node(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = terminal_node(left), terminal_node(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = terminal_node(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = best_get_split(left, k_features)\n",
    "\t\tget_split(node['left'], max_depth, min_size, k_features, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = terminal_node(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = best_get_split(right, k_features)\n",
    "\t\tget_split(node['right'], max_depth, min_size, k_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def tree_build(train, max_depth, min_size, k_features):\n",
    "\troot = best_get_split(train, k_features)\n",
    "\tget_split(root, max_depth, min_size, k_features, 1)\n",
    "\treturn root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def forecast(node, col):\n",
    "\tif col[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn forecast(node['left'], col)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn forecast(node['right'], col)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataframe, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataframe) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataframe))\n",
    "\t\tsample.append(dataframe[index])\n",
    "\treturn sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_prediction(trees, col):\n",
    "\tpredictions = [forecast(tree, col) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest_algo(train, test, max_depth, min_size, sample_size, n_trees, k_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = tree_build(sample, max_depth, min_size, k_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_prediction(trees, col) for col in test]\n",
    "\treturn(predictions)\n",
    "\n",
    "# OOB forecast Algorithm\n",
    "def OOB_forecast(train, test, max_depth, min_size, sample_size, n_trees, k_features):\n",
    "    trees = list()\n",
    "    OOB_scores=list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        OOB_sample=[i for i in train if i not in sample]\n",
    "        OOB_actual=[col[-1] for col in OOB_sample]\n",
    "        tree = tree_build(sample, max_depth, min_size, k_features)\n",
    "        trees.append(tree)\n",
    "        OOB_predict=[bagging_prediction(trees,col) for col in OOB_sample]\n",
    "        OOB_score=accuracy_score(OOB_actual,OOB_predict)\n",
    "        OOB_scores.append(OOB_score)\n",
    "    #print(\"OOB Scores\",OOB_scores)\n",
    "    return(sum(OOB_scores)/len(OOB_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the spam dataset\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"spam.data.txt\",header=None, delimiter=\" \")\n",
    "df=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyper-parameters\n",
    "max_depth = 10\n",
    "min_size = 10\n",
    "sample_size = 0.66\n",
    "k_features = int(sqrt(len(df[0])-1))\n",
    "trees=[1,5,10,15,20]\n",
    "scores=list()\n",
    "test=list()\n",
    "OOB_scores=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: 89.50036205648081\n",
      "OOB Scores: 86.21815806662312\n",
      "Trees: 5\n",
      "Scores: 93.70021723388848\n",
      "OOB Scores: 89.77394390698862\n",
      "Trees: 10\n",
      "Scores: 93.62780593772628\n",
      "OOB Scores: 92.56036118660295\n",
      "Trees: 15\n",
      "Scores: 94.6415640839971\n",
      "OOB Scores: 93.00215839201638\n",
      "Trees: 20\n",
      "Scores: 95.07603186097032\n",
      "OOB Scores: 93.1606641431746\n"
     ]
    }
   ],
   "source": [
    "#Running the defined Random Forest model and calculating OOB errors\n",
    "from sklearn.model_selection import train_test_split\n",
    "for n_trees in trees:\n",
    "    train,test= train_test_split(df, test_size=0.3, random_state=3250)\n",
    "    actual = [col[-1] for col in test]\n",
    "    predicted=random_forest_algo(train, test, max_depth, min_size, sample_size, n_trees, k_features)\n",
    "    OOB_score=OOB_forecast(train, test, max_depth, min_size, sample_size, n_trees, k_features)\n",
    "    OOB_scores.append(OOB_score)\n",
    "    score=accuracy_score(actual, predicted)\n",
    "    scores.append(score)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % score)\n",
    "    print('OOB Scores: %s' % OOB_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features(m): 1\n",
      "Scores: 88.34178131788559\n",
      "OOB Scores: 79.1377793217569\n",
      "No. of features(m): 5\n",
      "Scores: 93.9174511223751\n",
      "OOB Scores: 88.33371518335733\n",
      "No. of features(m): 10\n",
      "Scores: 94.27950760318609\n",
      "OOB Scores: 90.82312485559571\n",
      "No. of features(m): 15\n",
      "Scores: 93.19333816075309\n",
      "OOB Scores: 91.68319823114079\n",
      "No. of features(m): 20\n",
      "Scores: 94.1346850108617\n",
      "OOB Scores: 92.17811496919562\n"
     ]
    }
   ],
   "source": [
    "#checking the sensitivity when changing the m parameters\n",
    "\n",
    "#defining the hyper-parameters\n",
    "max_depth = 10\n",
    "min_size = 10\n",
    "sample_size = 0.66\n",
    "m_paramter=[1,5,10,15,20]\n",
    "scores=list()\n",
    "test=list()\n",
    "OOB_scores=list()\n",
    "\n",
    "#Running the defined Random Forest model and calculating OOB errors\n",
    "from sklearn.model_selection import train_test_split\n",
    "for m in m_paramter:\n",
    "    train,test= train_test_split(df, test_size=0.3, random_state=3250)\n",
    "    actual = [col[-1] for col in test]\n",
    "    predicted=random_forest_algo(train, test, max_depth, min_size, sample_size, 5, m)\n",
    "    OOB_score=OOB_forecast(train, test, max_depth, min_size, sample_size, 5, m)\n",
    "    OOB_scores.append(OOB_score)\n",
    "    score=accuracy_score(actual, predicted)\n",
    "    scores.append(score)\n",
    "    print('No. of features(m): %d' % m)\n",
    "    print('Scores: %s' % score)\n",
    "    print('OOB Scores: %s' % OOB_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"spam.data.txt\",header=None, delimiter=\" \")\n",
    "y=df[df.columns[-1]]\n",
    "x=df.drop(df.columns[-1],axis=1)\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y, test_size=0.3, random_state=3250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9601737871107893"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using Scikit Learn Random Forecast for classification\n",
    "rf_sklearn=RandomForestClassifier()\n",
    "rf_sklearn.fit(x_train,y_train)\n",
    "predicted=rf_sklearn.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
